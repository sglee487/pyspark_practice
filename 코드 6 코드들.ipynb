{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "strategic-museum",
   "metadata": {},
   "source": [
    "http://reader.epubee.com/books/mobile/bd/bd23bdcc487c79426075ac7c0e46c4e4/text00026.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "behavioral-alexander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.8.5 (default, Sep  4 2020 02:22:02)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "SPARK_HOME = '/Users/isang-geon/spark'\n",
    "# exec(open(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')).read())\n",
    "exec(open(os.path.join(SPARK_HOME, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "architectural-passing",
   "metadata": {},
   "source": [
    "코드 6.2 하이브 지원으로 SparkSession 객체 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-sugar",
   "metadata": {},
   "source": [
    "https://statkclee.github.io/bigdata/bigdata-spark-sql.html  \n",
    "https://spark.apache.org/docs/latest/sql-migration-guide.html  \n",
    "https://charsyam.wordpress.com/2020/10/30/%EC%9E%85-%EA%B0%9C%EB%B0%9C-spark%EC%97%90%EC%84%9C-parquet-%ED%8C%8C%EC%9D%BC-custom-schema-%EB%A1%9C-%EC%9D%BD%EC%96%B4%EB%93%A4%EC%9D%B4%EA%B8%B0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civil-glass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|    8/7/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"station_id\",IntegerType(),True) \\\n",
    "      .add(\"name\",StringType(),True) \\\n",
    "      .add(\"lat\",DoubleType(),True) \\\n",
    "      .add(\"long\",DoubleType(),True) \\\n",
    "      .add(\"dockcount\",IntegerType(),True) \\\n",
    "      .add(\"landmark\",StringType(),True) \\\n",
    "      .add(\"installation\",StringType(),True)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "stations = spark.read.schema(schema).csv(SPARK_HOME + \"/data/bike-share/stations/stations.csv\", inferSchema = True, header = False)\n",
    "# iris_df = spark.read.csv(SPARK_HOME + \"/data/bike-share/stations/stations.csv\", inferSchema = True, header = False)\n",
    "\n",
    "stations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vertical-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|    8/7/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations.createOrReplaceTempView(\"stations_table\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM stations_table LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fuzzy-exhaust",
   "metadata": {},
   "source": [
    "코드 6.3 스파크 SQL을 사용한 하이브 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pleasant-fusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+\n",
      "|                name|      lat|       long|\n",
      "+--------------------+---------+-----------+\n",
      "|San Jose Diridon ...|37.329732|-121.901782|\n",
      "|San Jose Civic Ce...|37.330698|-121.888979|\n",
      "|Santa Clara at Al...|37.333988|-121.894902|\n",
      "|    Adobe on Almaden|37.331415|  -121.8932|\n",
      "|    San Pedro Square|37.336721|-121.894074|\n",
      "|Paseo de San Antonio|37.333798|-121.886943|\n",
      "| San Salvador at 1st|37.330165|-121.885831|\n",
      "|           Japantown|37.348742|-121.894715|\n",
      "|  San Jose City Hall|37.337391|-121.886995|\n",
      "|         MLK Library|37.335885| -121.88566|\n",
      "|SJSU 4th at San C...|37.332808|-121.883891|\n",
      "|       St James Park|37.339301|-121.889937|\n",
      "|Arena Green / SAP...|37.332692|-121.900084|\n",
      "|SJSU - San Salvad...|37.333955|-121.877349|\n",
      "|Santa Clara Count...|37.352601|-121.905733|\n",
      "|         Ryland Park|37.342725|-121.895617|\n",
      "+--------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT name, lat, long FROM stations_table WHERE landmark = 'San Jose'\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-prime",
   "metadata": {},
   "source": [
    "↓↓↓↓↓↓↓↓↓========================================================↓↓↓↓↓↓↓↓↓↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "실패한 것들인데 쓸만한것 같아 남겨둠"
   ]
  },
  {
   "cell_type": "raw",
   "id": "revolutionary-memorabilia",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
    "http://jason-heo.github.io/bigdata/2019/07/14/spark-csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceramic-monitoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 2: string (nullable = true)\n",
      " |-- San Jose Diridon Caltrain Station: string (nullable = true)\n",
      " |-- 37.329732: string (nullable = true)\n",
      " |-- -121.901782: string (nullable = true)\n",
      " |-- 27: string (nullable = true)\n",
      " |-- San Jose: string (nullable = true)\n",
      " |-- 8/6/2013: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col,array_contains\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"station_id\",IntegerType(),True) \\\n",
    "      .add(\"name\",StringType(),True) \\\n",
    "      .add(\"lat\",DoubleType(),True) \\\n",
    "      .add(\"long\",DoubleType(),True) \\\n",
    "      .add(\"dockcount\",IntegerType(),True) \\\n",
    "      .add(\"landmark\",StringType(),True) \\\n",
    "      .add(\"installation\",StringType(),True)\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "df3 = spark.read.options(header='True', delimiter=',') \\\n",
    "  .csv(SPARK_HOME + \"/data/bike-share/stations/stations.csv\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "athletic-coupon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(SPARK_HOME + \"/data/bike-share/stations/stations.csv\")\n",
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "double-handle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|     landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|     San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|     San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|     San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|     San Jose|    8/7/2013|\n",
      "|         7|Paseo de San Antonio|37.333798|-121.886943|       15|     San Jose|    8/7/2013|\n",
      "|         8| San Salvador at 1st|37.330165|-121.885831|       15|     San Jose|    8/5/2013|\n",
      "|         9|           Japantown|37.348742|-121.894715|       15|     San Jose|    8/5/2013|\n",
      "|        10|  San Jose City Hall|37.337391|-121.886995|       15|     San Jose|    8/6/2013|\n",
      "|        11|         MLK Library|37.335885| -121.88566|       19|     San Jose|    8/6/2013|\n",
      "|        12|SJSU 4th at San C...|37.332808|-121.883891|       19|     San Jose|    8/7/2013|\n",
      "|        13|       St James Park|37.339301|-121.889937|       15|     San Jose|    8/6/2013|\n",
      "|        14|Arena Green / SAP...|37.332692|-121.900084|       19|     San Jose|    8/5/2013|\n",
      "|        16|SJSU - San Salvad...|37.333955|-121.877349|       15|     San Jose|    8/7/2013|\n",
      "|        21|   Franklin at Maple|37.481758|-122.226904|       15| Redwood City|   8/12/2013|\n",
      "|        22|Redwood City Calt...|37.486078|-122.232089|       25| Redwood City|   8/15/2013|\n",
      "|        23|San Mateo County ...|37.487616|-122.229951|       15| Redwood City|   8/15/2013|\n",
      "|        24|Redwood City Publ...|37.484219|-122.227424|       15| Redwood City|   8/12/2013|\n",
      "|        25|Stanford in Redwo...| 37.48537|-122.203288|       15| Redwood City|   8/12/2013|\n",
      "|        26|Redwood City Medi...|37.487682|-122.223492|       15| Redwood City|   8/12/2013|\n",
      "|        27|Mountain View Cit...|37.389218|-122.081896|       15|Mountain View|   8/16/2013|\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-southeast",
   "metadata": {},
   "source": [
    "↑↑↑↑↑↑↑=================================================↑↑↑↑↑↑↑↑↑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-genius",
   "metadata": {},
   "source": [
    "코드 6.5 하이브의 테이블에서 데이터프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convinced-point",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_cmd = \"\"\"SELECT name, lat, long\n",
    "FROM stations_table\n",
    "WHERE landmark = 'San Jose'\"\"\"\n",
    "\n",
    "df = spark.sql(sql_cmd)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "standard-poverty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+\n",
      "|                name|      lat|       long|\n",
      "+--------------------+---------+-----------+\n",
      "|San Jose Diridon ...|37.329732|-121.901782|\n",
      "|San Jose Civic Ce...|37.330698|-121.888979|\n",
      "|Santa Clara at Al...|37.333988|-121.894902|\n",
      "|    Adobe on Almaden|37.331415|  -121.8932|\n",
      "|    San Pedro Square|37.336721|-121.894074|\n",
      "+--------------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-paris",
   "metadata": {},
   "source": [
    "코드 6.6 하이브의 테이블에서 데이터프레임을 만드는 table() 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "placed-capacity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['station_id', 'name', 'lat', 'long', 'dockcount', 'landmark', 'installation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.table('stations_table')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expected-happiness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-meaning",
   "metadata": {},
   "source": [
    "코드 6.7 JSON 파일에서 데이터프레임을 만드는 read.json() 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "moderate-exception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_json_file = SPARK_HOME + '/examples/src/main/resources/people.json'\n",
    "people_df = spark.read.json(people_json_file)\n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-bryan",
   "metadata": {},
   "source": [
    "코드 6.8 JSON RDD에 데이터프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "simple-anthony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------------+\n",
      "|      lat|       long|            name|\n",
      "+---------+-----------+----------------+\n",
      "|37.331415|  -121.8932|Adobe on Almaden|\n",
      "|37.348742|-121.894715|       Japantown|\n",
      "+---------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize( \\\n",
    "                    ['{\"name\":\"Adobe on Almaden\", \"lat\":37.331415, \"long\":-121.8932}', \\\n",
    "                    '{\"name\":\"Japantown\", \"lat\":37.348742, \"long\":-121.894715}'])\n",
    "json_df = spark.read.json(rdd)\n",
    "json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-middle",
   "metadata": {},
   "source": [
    "코드 6.9 하나 이상의 플레인 텍스트 파일에서 데이터프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "antique-rwanda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='2,San Jose Diridon Caltrain Station,37.329732,-121.901782,27,San Jose,8/6/2013')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read an individual file\n",
    "df = spark.read.text('file://' + SPARK_HOME + '/data/bike-share/stations/stations.csv')\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "certified-sleeve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also read all files from a directory...\n",
    "df = spark.read.text('file://' + SPARK_HOME + '/data/bike-share/stations/')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-intellectual",
   "metadata": {},
   "source": [
    "코드 6.10 하나 이상의 파케이 파일에서 데이터프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cultural-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = spark.read.parquet('hdfs://' + SPARK_HOME + '/hadoopdata/stations.parquet')\n",
    "# df = spark.read.parquet('file://' + SPARK_HOME + '/hadoopdata/stations.parquet')\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "directed-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-prediction",
   "metadata": {},
   "source": [
    "코드 6.11 하이브 ORC 파일에서 데이터프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "damaged-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = spark.read.parquet('hdfs://' + SPARK_HOME + '/hadoopdata/stations_orc')\n",
    "# df = spark.read.parquet('file://' + SPARK_HOME + '/hadoopdata/stations_orc')\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "breathing-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-gibraltar",
   "metadata": {},
   "source": [
    "코드 6.12 데이터프레임을 RDD로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "warming-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stationsdf = spark.read.parquet('hdfs://' + SPARK_HOME + '/hadoopdata/stations.parquet')\n",
    "# stationsdf = spark.read.parquet('file://' + SPARK_HOME + '/hadoopdata/stations.parquet')\n",
    "# stationsrdd = stationsdf.rdd\n",
    "# stationsrdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "verbal-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationsrdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-password",
   "metadata": {},
   "source": [
    "코드 6.13 RDD에서 생성된 데이터프레임의 스키마 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "continental-index",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  'San Jose Diridon Caltrain Station',\n",
       "  37.329732,\n",
       "  -121.901782,\n",
       "  27,\n",
       "  'San Jose',\n",
       "  '8/6/2013'),\n",
       " (3,\n",
       "  'San Jose Civic Center',\n",
       "  37.330698,\n",
       "  -121.888979,\n",
       "  15,\n",
       "  'San Jose',\n",
       "  '8/5/2013')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('file://' + SPARK_HOME + '/data/bike-share/stations/stations.csv') \\\n",
    "        .map(lambda x: x.split(',')) \\\n",
    "        .map(lambda x:(int(x[0]), str(x[1]),\n",
    "                      float(x[2]), float(x[3]),\n",
    "                      int(x[4]), str(x[5]), str(x[6])))\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "european-cathedral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: double (nullable = true)\n",
      " |-- _4: double (nullable = true)\n",
      " |-- _5: long (nullable = true)\n",
      " |-- _6: string (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(rdd)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-webcam",
   "metadata": {},
   "source": [
    "스키마 추론은 JSON 문서에서 생성된 데이터프레임에 대해 자동으로 수행된다. (코드 6.14)  \n",
    "코드 6.14 JSON 객체에서 생성된 데이터프레임에 대한 스키마 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "twelve-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize( \\\n",
    "                    ['{\"name\":\"Adobe on Almaden\", \"lat\":37.331415, \"long\":-121.8932}', \\\n",
    "                    '{\"name\":\"Japantown\", \"lat\":37.348742, \"long\":-121.894715}'])\n",
    "df = spark.read.json(rdd)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-organizer",
   "metadata": {},
   "source": [
    "하이브 테이블에서 생성된 데이터프레임의 스키마는 하이브  정의로부터 자동으로 상속된다. (코드 6.15)  \n",
    "코드 6.15 하이브 테이블에서 생성된 데이터프레임 스키마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "double-westminster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"stations_table\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-exhibition",
   "metadata": {},
   "source": [
    "코드 6.16 명시적으로 데이터프레임에 대한 스키마 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "visible-retrieval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- long: float (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "myschema = StructType([ \\\n",
    "                      StructField(\"station_id\", IntegerType(), True), \\\n",
    "                      StructField(\"name\", StringType(), True), \\\n",
    "                      StructField(\"lat\", FloatType(), True), \\\n",
    "                      StructField(\"long\", FloatType(), True), \\\n",
    "                      StructField(\"dockcount\", IntegerType(), True), \\\n",
    "                      StructField(\"landmark\", StringType(), True), \\\n",
    "                      StructField(\"installation\", StringType(), True), \\\n",
    "                      ])\n",
    "rdd = sc.textFile(\"file://\" + SPARK_HOME + '/data/bike-share/stations/stations.csv') \\\n",
    "        .map(lambda x: x.split(',')) \\\n",
    "        .map(lambda x:(int(x[0]), str(x[1]),\n",
    "                      float(x[2]), float(x[3]),\n",
    "                      int(x[4]), str(x[5]), str(x[6])))\n",
    "df = spark.createDataFrame(rdd, myschema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-dependence",
   "metadata": {},
   "source": [
    "코드 6.23 스파크 SQL 데이터프레임을 사용한 map() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = sqlContext.read.parquet('hdfs:///.../stations.parquet')\n",
    "# rdd = df.map (lambda r: r.name)\n",
    "# rdd\n",
    "# PythonRDD[11] at collect at<stdin>:1\n",
    "# rdd.take(1)\n",
    "# [u'San Jose Diridon Caltrain Station']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-practice",
   "metadata": {},
   "source": [
    "select()는 새 데이터프레임을 반환하지만, map과 flatMap은 새 RDD를 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-spell",
   "metadata": {},
   "source": [
    "코드 6.25 스파크 SQL의 사용자 정의 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.parquet('hdfs:///.../stations.parquet')\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *\n",
    "# lat2dir = udf (lambda x: 'N' if x > 0 else 'S', StringType())\n",
    "# lon2dir = udf (lambda x: 'E' if x > 0 else 'W', StringType())\n",
    "# df.select(df.lat, lat2dir(df.lat).alias('latdir'),\n",
    "#           df.long, lon2dir(df.lat).alias('longdir')) \\\n",
    "#          .show()\n",
    "# +---------+------+-----------+-------+\n",
    "# |      lat|latdir|       long|longdir|\n",
    "# +---------+------+-----------+-------+\n",
    "# |37.329732|     N|-121.901782|      E|\n",
    "# |37.330698|     N|-121.888979|      E|\n",
    "# |   ...   |  ... |    ...    |  ...  |\n",
    "# +---------+------+-----------+-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-sullivan",
   "metadata": {},
   "source": [
    "코드 6.28 데이터프레임의 데이터 그룹화 및 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trips = spark.table(\"trips\")\n",
    "# averaged = trips.groupBy ([trips.start_terminal]).avg('duration') \\\n",
    "#                 .show(2)\n",
    "# +--------------+------------------+\n",
    "# |start_terminal|     avg(duration)|\n",
    "# +--------------+------------------+\n",
    "# |            31|2747.6333021515434|\n",
    "# |            32|1676.1081300813007|\n",
    "# +--------------+------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-transmission",
   "metadata": {},
   "source": [
    "코드 6.29 하이브 테이블에 데이터프레임 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "renewable-august",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|station_id|                name|\n",
      "+----------+--------------------+\n",
      "|         2|San Jose Diridon ...|\n",
      "|         3|San Jose Civic Ce...|\n",
      "+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tations = spark.table(\"stations_table\")\n",
    "stations.select([stations.station_id,stations.name]).write \\\n",
    "        .saveAsTable (\"station_names\")\n",
    "# load new table\n",
    "station_names = spark.table(\"station_names\")\n",
    "station_names.show(2)\n",
    "# +----------+--------------------+\n",
    "# |station_id|                name|\n",
    "# +----------+--------------------+\n",
    "# |         2|San Jose Diridon ...|\n",
    "# |         3|San Jose Civic Ce...|\n",
    "# +----------+--------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-rings",
   "metadata": {},
   "source": [
    "코드 6.30 CSV 파일에 데이터프레임 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.table(\"stations_table\") \\\n",
    "#         .write.csv(\"stations_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-suggestion",
   "metadata": {},
   "source": [
    "코드 6.31 파케이 파일에 데이터프레임 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "equivalent-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#                     .config(\"spark.sql.parquet.compression.codec.\", \"snappy\") \\\n",
    "#                     .getOrCreate()\n",
    "# stations = spark.table(\"stations_table\")\n",
    "# stations.select([stations.station_id,stations.name]).write \\\n",
    "#         .parquet(\"file://\" + SPARK_HOME + \"/hadoopdata/stations.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-marker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
