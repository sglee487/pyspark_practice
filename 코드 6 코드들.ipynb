{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worst-digit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.8.5 (default, Sep  4 2020 02:22:02)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "SPARK_HOME = '/Users/isang-geon/spark'\n",
    "# exec(open(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')).read())\n",
    "exec(open(os.path.join(SPARK_HOME, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "instructional-trademark",
   "metadata": {},
   "source": [
    "코드 6.2 하이브 지원으로 SparkSession 객체 생성하기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "mental-terrace",
   "metadata": {},
   "source": [
    "https://statkclee.github.io/bigdata/bigdata-spark-sql.html\n",
    "https://spark.apache.org/docs/latest/sql-migration-guide.html\n",
    "https://charsyam.wordpress.com/2020/10/30/%EC%9E%85-%EA%B0%9C%EB%B0%9C-spark%EC%97%90%EC%84%9C-parquet-%ED%8C%8C%EC%9D%BC-custom-schema-%EB%A1%9C-%EC%9D%BD%EC%96%B4%EB%93%A4%EC%9D%B4%EA%B8%B0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "whole-diversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|    8/7/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"station_id\",IntegerType(),True) \\\n",
    "      .add(\"name\",StringType(),True) \\\n",
    "      .add(\"lat\",DoubleType(),True) \\\n",
    "      .add(\"long\",DoubleType(),True) \\\n",
    "      .add(\"dockcount\",IntegerType(),True) \\\n",
    "      .add(\"landmark\",StringType(),True) \\\n",
    "      .add(\"installation\",StringType(),True)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "stations = spark.read.schema(schema).csv(SPARK_HOME + \"/data/bike-share/stations/stations.csv\", inferSchema = True, header = False)\n",
    "# iris_df = spark.read.csv(SPARK_HOME + \"/data/bike-share/stations/stations.csv\", inferSchema = True, header = False)\n",
    "\n",
    "stations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "macro-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|    8/7/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations.createOrReplaceTempView(\"stations_table\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM stations_table LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "composite-belarus",
   "metadata": {},
   "source": [
    "코드 6.3 스파크 SQL을 사용한 하이브 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "paperback-speech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+\n",
      "|                name|      lat|       long|\n",
      "+--------------------+---------+-----------+\n",
      "|San Jose Diridon ...|37.329732|-121.901782|\n",
      "|San Jose Civic Ce...|37.330698|-121.888979|\n",
      "|Santa Clara at Al...|37.333988|-121.894902|\n",
      "|    Adobe on Almaden|37.331415|  -121.8932|\n",
      "|    San Pedro Square|37.336721|-121.894074|\n",
      "|Paseo de San Antonio|37.333798|-121.886943|\n",
      "| San Salvador at 1st|37.330165|-121.885831|\n",
      "|           Japantown|37.348742|-121.894715|\n",
      "|  San Jose City Hall|37.337391|-121.886995|\n",
      "|         MLK Library|37.335885| -121.88566|\n",
      "|SJSU 4th at San C...|37.332808|-121.883891|\n",
      "|       St James Park|37.339301|-121.889937|\n",
      "|Arena Green / SAP...|37.332692|-121.900084|\n",
      "|SJSU - San Salvad...|37.333955|-121.877349|\n",
      "|Santa Clara Count...|37.352601|-121.905733|\n",
      "|         Ryland Park|37.342725|-121.895617|\n",
      "+--------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT name, lat, long FROM stations_table WHERE landmark = 'San Jose'\"\"\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "residential-civilization",
   "metadata": {},
   "source": [
    "↓↓↓↓↓↓↓↓↓========================================================↓↓↓↓↓↓↓↓↓↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "실패한 것들인데 쓸만한것 같아 남겨둠"
   ]
  },
  {
   "cell_type": "raw",
   "id": "involved-universal",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
    "http://jason-heo.github.io/bigdata/2019/07/14/spark-csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "established-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 2: string (nullable = true)\n",
      " |-- San Jose Diridon Caltrain Station: string (nullable = true)\n",
      " |-- 37.329732: string (nullable = true)\n",
      " |-- -121.901782: string (nullable = true)\n",
      " |-- 27: string (nullable = true)\n",
      " |-- San Jose: string (nullable = true)\n",
      " |-- 8/6/2013: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col,array_contains\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"station_id\",IntegerType(),True) \\\n",
    "      .add(\"name\",StringType(),True) \\\n",
    "      .add(\"lat\",DoubleType(),True) \\\n",
    "      .add(\"long\",DoubleType(),True) \\\n",
    "      .add(\"dockcount\",IntegerType(),True) \\\n",
    "      .add(\"landmark\",StringType(),True) \\\n",
    "      .add(\"installation\",StringType(),True)\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "df3 = spark.read.options(header='True', delimiter=',') \\\n",
    "  .csv(SPARK_HOME + \"/data/bike-share/stations/stations.csv\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "following-american",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(SPARK_HOME + \"/data/bike-share/stations/stations.csv\")\n",
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "usual-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|     landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|     San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|     San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|     San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|     San Jose|    8/7/2013|\n",
      "|         7|Paseo de San Antonio|37.333798|-121.886943|       15|     San Jose|    8/7/2013|\n",
      "|         8| San Salvador at 1st|37.330165|-121.885831|       15|     San Jose|    8/5/2013|\n",
      "|         9|           Japantown|37.348742|-121.894715|       15|     San Jose|    8/5/2013|\n",
      "|        10|  San Jose City Hall|37.337391|-121.886995|       15|     San Jose|    8/6/2013|\n",
      "|        11|         MLK Library|37.335885| -121.88566|       19|     San Jose|    8/6/2013|\n",
      "|        12|SJSU 4th at San C...|37.332808|-121.883891|       19|     San Jose|    8/7/2013|\n",
      "|        13|       St James Park|37.339301|-121.889937|       15|     San Jose|    8/6/2013|\n",
      "|        14|Arena Green / SAP...|37.332692|-121.900084|       19|     San Jose|    8/5/2013|\n",
      "|        16|SJSU - San Salvad...|37.333955|-121.877349|       15|     San Jose|    8/7/2013|\n",
      "|        21|   Franklin at Maple|37.481758|-122.226904|       15| Redwood City|   8/12/2013|\n",
      "|        22|Redwood City Calt...|37.486078|-122.232089|       25| Redwood City|   8/15/2013|\n",
      "|        23|San Mateo County ...|37.487616|-122.229951|       15| Redwood City|   8/15/2013|\n",
      "|        24|Redwood City Publ...|37.484219|-122.227424|       15| Redwood City|   8/12/2013|\n",
      "|        25|Stanford in Redwo...| 37.48537|-122.203288|       15| Redwood City|   8/12/2013|\n",
      "|        26|Redwood City Medi...|37.487682|-122.223492|       15| Redwood City|   8/12/2013|\n",
      "|        27|Mountain View Cit...|37.389218|-122.081896|       15|Mountain View|   8/16/2013|\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "opposite-kennedy",
   "metadata": {},
   "source": [
    "↑↑↑↑↑↑↑=================================================↑↑↑↑↑↑↑↑↑"
   ]
  },
  {
   "cell_type": "raw",
   "id": "laden-trust",
   "metadata": {},
   "source": [
    "코드 6.5 하이브의 테이블에서 데이터프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "equal-spyware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_cmd = \"\"\"SELECT name, lat, long\n",
    "FROM stations_table\n",
    "WHERE landmark = 'San Jose'\"\"\"\n",
    "\n",
    "df = spark.sql(sql_cmd)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "architectural-disposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+\n",
      "|                name|      lat|       long|\n",
      "+--------------------+---------+-----------+\n",
      "|San Jose Diridon ...|37.329732|-121.901782|\n",
      "|San Jose Civic Ce...|37.330698|-121.888979|\n",
      "|Santa Clara at Al...|37.333988|-121.894902|\n",
      "|    Adobe on Almaden|37.331415|  -121.8932|\n",
      "|    San Pedro Square|37.336721|-121.894074|\n",
      "+--------------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "engaging-desert",
   "metadata": {},
   "source": [
    "코드 6.6 하이브의 테이블에서 데이터프레임을 만드는 table() 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addressed-cover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['station_id', 'name', 'lat', 'long', 'dockcount', 'landmark', 'installation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.table('stations_table')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "referenced-handbook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sitting-bumper",
   "metadata": {},
   "source": [
    "코드 6.7 JSON 파일에서 데이터프레임을 만드는 read.json() 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hawaiian-minutes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_json_file = SPARK_HOME + '/examples/src/main/resources/people.json'\n",
    "people_df = spark.read.json(people_json_file)\n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "demographic-museum",
   "metadata": {},
   "source": [
    "코드 6.8 JSON RDD에 데이터프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "composed-convention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------------+\n",
      "|      lat|       long|            name|\n",
      "+---------+-----------+----------------+\n",
      "|37.331415|  -121.8932|Adobe on Almaden|\n",
      "|37.348742|-121.894715|       Japantown|\n",
      "+---------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize( \\\n",
    "                    ['{\"name\":\"Adobe on Almaden\", \"lat\":37.331415, \"long\":-121.8932}', \\\n",
    "                    '{\"name\":\"Japantown\", \"lat\":37.348742, \"long\":-121.894715}'])\n",
    "json_df = spark.read.json(rdd)\n",
    "json_df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "circular-answer",
   "metadata": {},
   "source": [
    "코드 6.9 하나 이상의 플레인 텍스트 파일에서 데이터프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "random-timer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='2,San Jose Diridon Caltrain Station,37.329732,-121.901782,27,San Jose,8/6/2013')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read an individual file\n",
    "df = spark.read.text('file://' + SPARK_HOME + '/data/bike-share/stations/stations.csv')\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "gross-catch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also read all files from a directory...\n",
    "df = spark.read.text('file://' + SPARK_HOME + '/data/bike-share/stations/')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "interim-channel",
   "metadata": {},
   "source": [
    "코드 6.10 하나 이상의 파케이 파일에서 데이터프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = spark.read.parquet('hdfs://' + SPARK_HOME + '/hadoopdata/stations.parquet')\n",
    "# df = spark.read.parquet('file://' + SPARK_HOME + '/hadoopdata/stations.parquet')\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.take(1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "subsequent-british",
   "metadata": {},
   "source": [
    "코드 6.11 하이브 ORC 파일에서 데이터프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "right-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = spark.read.parquet('hdfs://' + SPARK_HOME + '/hadoopdata/stations_orc')\n",
    "# df = spark.read.parquet('file://' + SPARK_HOME + '/hadoopdata/stations_orc')\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
